
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning to Discover at Test Time</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: 'Inter', -apple-system, sans-serif;
            background: #fff;
            color: #111;
            font-size: 14px;
            line-height: 1.6;
        }

        .page {
            max-width: 900px;
            margin: 0 auto;
            padding: 72px 32px;
        }

        header {
            text-align: center;
            margin-bottom: 56px;
        }

        .paper-title {
            font-size: 2.75rem;
            font-weight: 600;
            line-height: 1.1;
            letter-spacing: -0.035em;
            margin-bottom: 24px;
        }

        .authors {
            font-size: 14px;
            line-height: 1.9;
            margin-bottom: 6px;
        }

        .authors sup {
            font-size: 10px;
            margin-left: 1px;
        }

        .institutions {
            font-size: 12px;
            color: #777;
            margin-bottom: 32px;
        }

        .links {
            display: flex;
            justify-content: center;
            gap: 16px;
        }

        .links a {
            padding: 12px 40px;
            font-size: 14px;
            font-weight: 500;
            color: #111;
            text-decoration: none;
            border: 1.5px solid #111;
            transition: all 0.15s;
        }

        .links a:hover {
            background: #111;
            color: #fff;
        }

        .abstract {
            font-size: 17px;
            font-weight: 300;
            line-height: 1.75;
            color: #333;
            margin-bottom: 56px;
        }

        section {
            margin-bottom: 48px;
        }

        .label {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: #aaa;
            margin-bottom: 16px;
        }

        .figure img {
            width: 100%;
            display: block;
        }

        .caption {
            margin-top: 16px;
            font-size: 13px;
            line-height: 1.75;
            color: #555;
        }

        .table-wrap {
            overflow-x: auto;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 13px;
        }

        th, td {
            text-align: left;
            padding: 11px 14px;
            border-bottom: 1px solid #e5e5e5;
        }

        th {
            font-weight: 600;
            font-size: 12px;
        }

        th span {
            display: block;
            font-weight: 400;
            font-size: 10px;
            color: #999;
            margin-top: 2px;
        }

        tbody tr:last-child td {
            font-weight: 600;
            border-bottom: 2px solid #111;
        }

        .dim { color: #ccc; }

        /* Domain sections */
        .domain-section {
            margin-top: 48px;
            padding-top: 40px;
            border-top: 1px solid #e5e5e5;
            position: relative;
        }

        .domain-section::before {
            content: '';
            position: absolute;
            top: -1px;
            left: 0;
            width: 48px;
            height: 3px;
            background: #111;
        }

        .domain-title {
            font-size: 18px;
            font-weight: 600;
            margin-bottom: 8px;
        }

        .domain-desc {
            font-size: 13px;
            color: #666;
            margin-bottom: 20px;
        }

        .domain-desc a {
            color: #111;
        }

        .mini-table {
            margin-bottom: 0;
        }

        .mini-table th, .mini-table td {
            padding: 8px 12px;
            font-size: 12px;
        }

        .bibtex {
            font-family: 'SF Mono', 'Monaco', 'Menlo', monospace;
            font-size: 12px;
            line-height: 1.7;
            padding: 20px 24px;
            background: #f8f8f8;
            white-space: pre;
            overflow-x: auto;
        }

        .code-details {
            margin-top: 16px;
        }

        .code-details summary {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            font-size: 12px;
            font-weight: 500;
            color: #666;
            background: none;
            border: 1px solid #ddd;
            padding: 6px 12px;
            cursor: pointer;
            transition: all 0.15s;
            list-style: none;
        }

        .code-details summary::-webkit-details-marker {
            display: none;
        }

        .code-details summary::before {
            content: '▶';
            font-size: 10px;
            transition: transform 0.2s;
        }

        .code-details[open] summary::before {
            transform: rotate(90deg);
        }

        .code-details summary:hover {
            border-color: #111;
            color: #111;
        }

        .code-block {
            margin-top: 16px;
            background: #1a1a1a;
            border-radius: 6px;
            overflow: hidden;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 16px;
            background: #252525;
            font-size: 11px;
            color: #888;
        }

        .code-block pre {
            padding: 0;
            margin: 0;
            max-height: 400px;
            border-radius: 0;
        }

        .code-block pre code {
            display: block;
            padding: 16px;
            font-family: 'SF Mono', 'Monaco', 'Menlo', monospace;
            font-size: 11px;
            line-height: 1.6;
            overflow: auto;
            max-height: 400px;
        }

        .code-block .keyword { color: #c792ea; }
        .code-block .function { color: #82aaff; }
        .code-block .string { color: #c3e88d; }
        .code-block .comment { color: #676e95; }
        .code-block .number { color: #f78c6c; }

        .plot-container {
            margin-top: 16px;
            padding: 20px;
            background: #fafafa;
            border: 1px solid #eee;
            border-radius: 4px;
        }

        .plot-container img {
            width: 100%;
            display: block;
        }

        .plot-caption {
            margin-top: 12px;
            font-size: 12px;
            color: #666;
            text-align: center;
        }

        footer {
            margin-top: 48px;
            font-size: 11px;
            font-weight: 500;
            letter-spacing: 0.08em;
            color: #bbb;
        }

        @media (max-width: 700px) {
            .page { padding: 48px 20px; }
            .paper-title { font-size: 1.75rem; }
            .abstract { font-size: 15px; }
        }
    </style>
</head>
<body>
    <div class="page">
        <header>
            <h1 class="paper-title">Learning to Discover at Test Time</h1>
            <p class="authors">
                Mert Yuksekgonul<sup>*</sup>, Daniel Koceja<sup>*</sup>, Xinhao Li<sup>*</sup>, Federico Bianchi<sup>*</sup><br>
                Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou<sup>†</sup>, Carlos Guestrin<sup>†</sup>, Yu Sun<sup>*</sup>
            </p>
            <p class="institutions">Stanford · NVIDIA · Astera Institute · UC San Diego · Together AI</p>
            <div class="links">
                <a href="https://test-time-training.github.io/discover.pdf">Paper</a>
                <a href="https://github.com/test-time-training/discover" target="_blank">Code</a>
            </div>
        </header>

        <p class="abstract">
            We perform reinforcement learning at test time, allowing the LLM to continue training with experience specific to the problem at hand. TTT-Discover achieves new state-of-the-art across mathematics, GPU kernels, algorithms, and biology.
        </p>

        <section>
            <p class="label">Key Results</p>
            <div class="table-wrap">
                <table>
                    <thead>
                        <tr>
                            <th></th>
                            <th>Mathematics<span>Erdős Overlap ↓</span></th>
                            <th>Kernel A100<span>TriMul ↓</span></th>
                            <th>Kernel H100<span>TriMul ↓</span></th>
                            <th>Algorithms<span>AtCoder ↑</span></th>
                            <th>Biology<span>Denoising ↑</span></th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Best Human</td>
                            <td>0.380927</td>
                            <td>4531 μs</td>
                            <td>1371 μs</td>
                            <td>566,997</td>
                            <td>0.64</td>
                        </tr>
                        <tr>
                            <td>Prev. Best AI</td>
                            <td>0.380924</td>
                            <td class="dim">—</td>
                            <td class="dim">—</td>
                            <td>558,026</td>
                            <td class="dim">—</td>
                        </tr>
                        <tr>
                            <td>TTT-Discover</td>
                            <td>0.380876</td>
                            <td>2198 μs</td>
                            <td>1161 μs</td>
                            <td>567,062</td>
                            <td>0.71</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section>
            <p class="label">Figure 1</p>
            <div class="figure">
                <img src="figure1.svg" alt="TTT-Discover">
            </div>
            <p class="caption">
                TTT-Discover continues to train an LLM on a single problem at test time. \(\theta_i\) denotes the model weights at training step \(i\). We plot the reward distribution at step 0, 9, 24, and 49 (final), recorded while test-time training for the GPUMode TriMul competition. We generate 512 solutions at each step from \(\pi_{\theta_i}\). As training progresses, the LLM generates better solutions that ultimately surpass the prior art (best human). For comparison, we plot the reward distribution of best-of-\(N\) with the same total sampling budget.
            </p>
        </section>

        <div class="domain-section">
            <h3 class="domain-title">Mathematics</h3>
            <p class="domain-desc">Classic open problems in combinatorics and analysis. <a href="https://github.com/test-time-training/discover/blob/main/results/mathematics/Results.ipynb" target="_blank">Verify constructions →</a></p>
            <div class="table-wrap">
                <table class="mini-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>Erdős Min. Overlap ↓</th>
                            <th>Autocorr. (AC1) ↓</th>
                            <th>Autocorr. (AC2) ↑</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Best Human</td>
                            <td>0.380927</td>
                            <td>1.50973</td>
                            <td>0.9015</td>
                        </tr>
                        <tr>
                            <td>Prev. Best AI</td>
                            <td>0.380924</td>
                            <td>1.50314</td>
                            <td>0.9610</td>
                        </tr>
                        <tr>
                            <td>TTT-Discover</td>
                            <td>0.380876</td>
                            <td>1.50287</td>
                            <td>0.9591</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="plot-container" style="margin-top: 20px;">
                <img src="assets/erdos.png" alt="Erdős minimum overlap construction">
                <p class="plot-caption">Comparison of constructions for the Erdős minimum overlap problem</p>
            </div>
        </div>

        <div class="domain-section">
            <h3 class="domain-title">Kernel Engineering</h3>
            <p class="domain-desc">GPUMode TriMul competition for triangular matrix multiplication. <a href="https://www.gpumode.com/v2/leaderboard/496?tab=rankings" target="_blank">Verify on leaderboard →</a></p>
            <div class="table-wrap">
                <table class="mini-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>A100 ↓</th>
                            <th>H100 ↓</th>
                            <th>B200 ↓</th>
                            <th>MI300x ↓</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Best Human</td>
                            <td>4531 μs</td>
                            <td>1371 μs</td>
                            <td>1005 μs</td>
                            <td>2462 μs</td>
                        </tr>
                        <tr>
                            <td>Prev. Best AI</td>
                            <td class="dim">—</td>
                            <td class="dim">—</td>
                            <td class="dim">—</td>
                            <td class="dim">—</td>
                        </tr>
                        <tr>
                            <td>TTT-Discover</td>
                            <td>2198 μs</td>
                            <td>1161 μs</td>
                            <td>905 μs</td>
                            <td>1596 μs</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <details class="code-details">
                <summary>View H100 kernel (1161μs) · 470 lines</summary>
                <div class="code-block">
                    <div class="code-header">
                        <span>kernel.py</span>
                    </div>
                    <pre><code class="language-python">"""
Outgoing TriMul (AlphaFold‑3) – Triton accelerated forward pass.

The implementation follows the reference ``TriMul`` module but fuses the
expensive kernels:

1️⃣  Row‑wise LayerNorm over the last dimension (FP16 output, FP32 reduction).
2️⃣  Fused projection, gating and optional scalar mask:
    * left_proj, right_proj  = x_norm @ W_proj
    * left_gate, right_gate, out_gate = sigmoid(x_norm @ W_gate)
    * left  = left_proj  * left_gate  * mask
    * right = right_proj * right_gate * mask
3️⃣  Pairwise multiplication across the sequence dimension (batched GEMM on
    fp16 tensors).
4️⃣  Fused hidden‑dim LayerNorm → out‑gate multiplication → final linear
    projection (all in one kernel, FP16 matmul with FP32 accumulation).

The output tensor has shape ``[B, N, N, dim]`` and dtype ``float32``.
"""

from typing import Tuple, Dict
import torch
import triton
import triton.language as tl


# ----------------------------------------------------------------------
# 1) Row‑wise LayerNorm (FP16 output, FP32 accumulator)
# ----------------------------------------------------------------------
@triton.jit
def _row_ln_fp16_kernel(
    X_ptr, Y_ptr,          # (M, C) input / output
    w_ptr, b_ptr,          # LN weight & bias (fp32)
    M, C: tl.constexpr,    # rows, columns (C is compile‑time constant)
    eps,
    BLOCK_M: tl.constexpr,
    BLOCK_C: tl.constexpr,
):
    pid = tl.program_id(0)
    row_start = pid * BLOCK_M
    rows = row_start + tl.arange(0, BLOCK_M)
    row_mask = rows < M

    # ---------- mean / var (fp32) ----------
    sum_val = tl.zeros([BLOCK_M], dtype=tl.float32)
    sumsq_val = tl.zeros([BLOCK_M], dtype=tl.float32)

    for c in range(0, C, BLOCK_C):
        cur_c = c + tl.arange(0, BLOCK_C)
        col_mask = cur_c < C
        x = tl.load(
            X_ptr + rows[:, None] * C + cur_c[None, :],
            mask=row_mask[:, None] & col_mask[None, :],
            other=0.0,
        ).to(tl.float32)                     # (BLOCK_M, BLOCK_C)

        sum_val += tl.sum(x, axis=1)
        sumsq_val += tl.sum(x * x, axis=1)

    mean = sum_val / C
    var = sumsq_val / C - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # ---------- normalize + affine (fp16) ----------
    for c in range(0, C, BLOCK_C):
        cur_c = c + tl.arange(0, BLOCK_C)
        col_mask = cur_c < C
        x = tl.load(
            X_ptr + rows[:, None] * C + cur_c[None, :],
            mask=row_mask[:, None] & col_mask[None, :],
            other=0.0,
        ).to(tl.float32)

        y = (x - mean[:, None]) * inv_std[:, None]

        w = tl.load(w_ptr + cur_c, mask=col_mask, other=0.0)
        b = tl.load(b_ptr + cur_c, mask=col_mask, other=0.0)

        y = y * w[None, :] + b[None, :]
        tl.store(
            Y_ptr + rows[:, None] * C + cur_c[None, :],
            y.to(tl.float16),
            mask=row_mask[:, None] & col_mask[None, :],
        )


def _row_layernorm_fp16(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float = 1e-5,
) -> torch.Tensor:
    """Row‑wise LayerNorm over the last dim → FP16 output."""
    B, N, _, C = x.shape
    M = B * N * N
    x_flat = x.view(M, C).contiguous()
    y_flat = torch.empty((M, C), dtype=torch.float16, device=x.device)

    BLOCK_M = 128
    BLOCK_C = 128
    grid = lambda meta: (triton.cdiv(M, meta["BLOCK_M"]),)

    _row_ln_fp16_kernel[grid](
        x_flat,
        y_flat,
        weight,
        bias,
        M,
        C,
        eps,
        BLOCK_M=BLOCK_M,
        BLOCK_C=BLOCK_C,
        num_warps=8,
    )
    return y_flat.view(B, N, N, C)


# ----------------------------------------------------------------------
# 2) Fused projection + gating + optional mask
# ----------------------------------------------------------------------
@triton.jit
def _proj_gate_mask_kernel(
    x_ptr,                         # (M, C) fp16
    mask_ptr,                      # (M,)  fp16 (if MASKED==1)
    left_proj_w_ptr,               # (C, H) fp16
    left_gate_w_ptr,               # (C, H) fp16
    right_proj_w_ptr,              # (C, H) fp16
    right_gate_w_ptr,              # (C, H) fp16
    out_gate_w_ptr,                # (C, H) fp16
    left_ptr,                      # (B, H, N, N) fp16
    right_ptr,                     # (B, H, N, N) fp16
    out_gate_ptr,                  # (B, N, N, H) fp16
    M, N, C: tl.constexpr, H: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_H: tl.constexpr,
    BLOCK_K: tl.constexpr,
    MASKED: tl.constexpr,
):
    pid_m = tl.program_id(0)   # row block
    pid_h = tl.program_id(1)   # hidden block

    row_start = pid_m * BLOCK_M
    hid_start = pid_h * BLOCK_H

    rows = row_start + tl.arange(0, BLOCK_M)          # (BLOCK_M,)
    hids = hid_start + tl.arange(0, BLOCK_H)         # (BLOCK_H,)

    row_mask = rows < M
    hid_mask = hids < H

    # ---------------- mask (scalar per row) ----------------
    if MASKED:
        mask_val = tl.load(mask_ptr + rows, mask=row_mask, other=0.0).to(tl.float32)  # (BLOCK_M,)
    else:
        mask_val = tl.full([BLOCK_M], 1.0, dtype=tl.float32)

    # ---------------- accumulators (fp32) ------------------
    acc_lp = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32)  # left proj
    acc_lg = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32)  # left gate
    acc_rp = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32)  # right proj
    acc_rg = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32)  # right gate
    acc_og = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32)  # out gate

    for k in range(0, C, BLOCK_K):
        cur_k = k + tl.arange(0, BLOCK_K)
        k_mask = cur_k < C

        # input tile (fp16 → fp32)
        a = tl.load(
            x_ptr + rows[:, None] * C + cur_k[None, :],
            mask=row_mask[:, None] & k_mask[None, :],
            other=0.0,
        )   # (BLOCK_M, BLOCK_K) fp16

        # weight tiles (C,H) column‑major
        w_lp = tl.load(
            left_proj_w_ptr + cur_k[:, None] * H + hids[None, :],
            mask=k_mask[:, None] & hid_mask[None, :],
            other=0.0,
        )
        w_lg = tl.load(
            left_gate_w_ptr + cur_k[:, None] * H + hids[None, :],
            mask=k_mask[:, None] & hid_mask[None, :],
            other=0.0,
        )
        w_rp = tl.load(
            right_proj_w_ptr + cur_k[:, None] * H + hids[None, :],
            mask=k_mask[:, None] & hid_mask[None, :],
            other=0.0,
        )
        w_rg = tl.load(
            right_gate_w_ptr + cur_k[:, None] * H + hids[None, :],
            mask=k_mask[:, None] & hid_mask[None, :],
            other=0.0,
        )
        w_og = tl.load(
            out_gate_w_ptr + cur_k[:, None] * H + hids[None, :],
            mask=k_mask[:, None] & hid_mask[None, :],
            other=0.0,
        )

        # fp16·fp16 → fp32 dot products
        acc_lp += tl.dot(a, w_lp)
        acc_lg += tl.dot(a, w_lg)
        acc_rp += tl.dot(a, w_rp)
        acc_rg += tl.dot(a, w_rg)
        acc_og += tl.dot(a, w_og)

    # ---------------- sigmoid gates -------------------------
    left_gate  = 1.0 / (1.0 + tl.exp(-acc_lg))
    right_gate = 1.0 / (1.0 + tl.exp(-acc_rg))
    out_gate   = 1.0 / (1.0 + tl.exp(-acc_og))

    # ---------------- apply mask and per‑row gates ----------
    left_out  = acc_lp * left_gate * mask_val[:, None]
    right_out = acc_rp * right_gate * mask_val[:, None]

    # ---------------- store left/right (B,H,N,N) -------------
    N_sq = N * N
    b_idx = rows // N_sq
    rem   = rows - b_idx * N_sq
    i_idx = rem // N
    k_idx = rem - i_idx * N

    # layout for left/right: (B, H, N, N) → flat index:
    off = ((b_idx[:, None] * H + hids[None, :]) * N_sq) + i_idx[:, None] * N + k_idx[:, None]

    tl.store(
        left_ptr + off,
        left_out.to(tl.float16),
        mask=row_mask[:, None] & hid_mask[None, :],
    )
    tl.store(
        right_ptr + off,
        right_out.to(tl.float16),
        mask=row_mask[:, None] & hid_mask[None, :],
    )

    # ---------------- store out_gate (B,N,N,H) ---------------
    out_off = rows[:, None] * H + hids[None, :]
    tl.store(
        out_gate_ptr + out_off,
        out_gate.to(tl.float16),
        mask=row_mask[:, None] & hid_mask[None, :],
    )


# ----------------------------------------------------------------------
# 3) Fused hidden‑dim LayerNorm → out‑gate → final linear
# ----------------------------------------------------------------------
@triton.jit
def _ln_gate_out_linear_fused_kernel(
    hidden_ptr,           # (B*H*N*N,) fp16 flattened
    out_gate_ptr,         # (B*N*N*H,) fp16 flattened
    ln_w_ptr, ln_b_ptr,  # (H,) fp32
    w_out_ptr,            # (H, D) fp16
    out_ptr,              # (B, N, N, D) fp32
    B, N, H, D: tl.constexpr,
    eps: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_H: tl.constexpr,
    BLOCK_D: tl.constexpr,
):
    pid = tl.program_id(0)
    row_start = pid * BLOCK_M
    rows = row_start + tl.arange(0, BLOCK_M)                # flat index for (b,i,j)
    row_mask = rows < (B * N * N)

    N_sq = N * N
    b_idx = rows // N_sq
    rem = rows - b_idx * N_sq
    i_idx = rem // N
    j_idx = rem - i_idx * N

    # ----- load hidden slice (BLOCK_M, BLOCK_H) ------------
    hids = tl.arange(0, BLOCK_H)
    hid_mask = hids < H

    hidden_off = ((b_idx[:, None] * H + hids[None, :]) * N_sq) + i_idx[:, None] * N + j_idx[:, None]
    hidden_tile = tl.load(
        hidden_ptr + hidden_off,
        mask=row_mask[:, None] & hid_mask[None, :],
        other=0.0,
    )  # fp16
    hidden_fp32 = hidden_tile.to(tl.float32)

    # ----- mean / var across H (fp32) -----
    sum_val = tl.sum(hidden_fp32, axis=1)                     # (BLOCK_M,)
    sumsq_val = tl.sum(hidden_fp32 * hidden_fp32, axis=1)    # (BLOCK_M,)
    mean = sum_val / H
    var = sumsq_val / H - mean * mean
    inv_std = 1.0 / tl.sqrt(var + eps)

    # ----- LayerNorm (fp32) -----
    w_ln = tl.load(ln_w_ptr + hids, mask=hid_mask, other=0.0)   # (H,)
    b_ln = tl.load(ln_b_ptr + hids, mask=hid_mask, other=0.0)   # (H,)
    hidden_norm = (hidden_fp32 - mean[:, None]) * inv_std[:, None]
    hidden_norm = hidden_norm * w_ln[None, :] + b_ln[None, :]   # (BLOCK_M, BLOCK_H)

    # ----- out‑gate (fp32) -----
    out_gate_off = rows[:, None] * H + hids[None, :]
    out_gate_tile = tl.load(
        out_gate_ptr + out_gate_off,
        mask=row_mask[:, None] & hid_mask[None, :],
        other=0.0,
    ).to(tl.float32)                                            # (BLOCK_M, BLOCK_H)

    gated = hidden_norm * out_gate_tile                         # (BLOCK_M, BLOCK_H)

    # ----- final linear projection (fp16 matmul, fp32 acc) -----
    gated_fp16 = gated.to(tl.float16)

    for d0 in range(0, D, BLOCK_D):
        cols = d0 + tl.arange(0, BLOCK_D)
        col_mask = cols < D

        w_out = tl.load(
            w_out_ptr + hids[:, None] * D + cols[None, :],
            mask=hid_mask[:, None] & col_mask[None, :],
            other=0.0,
        )  # (BLOCK_H, BLOCK_D) fp16

        out = tl.dot(gated_fp16, w_out)                       # (BLOCK_M, BLOCK_D) fp32

        tl.store(
            out_ptr + rows[:, None] * D + cols[None, :],
            out,
            mask=row_mask[:, None] & col_mask[None, :],
        )


# ----------------------------------------------------------------------
# 4) Entrypoint
# ----------------------------------------------------------------------
def custom_kernel(
    data: Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor], Dict]
) -> torch.Tensor:
    """
    Forward pass of the outgoing TriMul operator (no gradients).

    Arguments
    ---------
    data : (input, mask, weights, config)
        - input  : Tensor[B, N, N, C] (float32)
        - mask   : Tensor[B, N, N] (bool/float) or None
        - weights: dict of module parameters (float32)
        - config : dict with ``dim`` (C) and ``hidden_dim`` (H) and optional ``nomask``

    Returns
    -------
    Tensor[B, N, N, C] (float32)
    """
    inp, mask, weights, cfg = data
    dim = cfg["dim"]                # C
    hidden_dim = cfg["hidden_dim"]  # H
    nomask = cfg.get("nomask", True)
    eps = 1e-5

    device = inp.device
    B, N, _, _ = inp.shape
    M = B * N * N                     # total rows for row‑wise ops

    # --------------------------------------------------------------
    # 1) Row‑wise LayerNorm (fp16 output)
    # --------------------------------------------------------------
    x_norm = _row_layernorm_fp16(
        inp,
        weights["norm.weight"],
        weights["norm.bias"],
        eps=eps,
    )  # (B, N, N, C) fp16

    # --------------------------------------------------------------
    # 2) Prepare projection / gate weights  (C, H) fp16, column‑major
    # --------------------------------------------------------------
    left_proj_w_T  = weights["left_proj.weight"].t().contiguous().to(torch.float16)
    right_proj_w_T = weights["right_proj.weight"].t().contiguous().to(torch.float16)
    left_gate_w_T  = weights["left_gate.weight"].t().contiguous().to(torch.float16)
    right_gate_w_T = weights["right_gate.weight"].t().contiguous().to(torch.float16)
    out_gate_w_T   = weights["out_gate.weight"].t().contiguous().to(torch.float16)

    # --------------------------------------------------------------
    # 3) Mask handling (optional)
    # --------------------------------------------------------------
    if not nomask and mask is not None:
        mask_flat = mask.reshape(M).to(torch.float16).contiguous()
        MASKED = 1
    else:
        mask_flat = torch.empty(0, dtype=torch.float16, device=device)
        MASKED = 0

    # --------------------------------------------------------------
    # 4) Allocate buffers for fused projection + gating
    # --------------------------------------------------------------
    left = torch.empty((B, hidden_dim, N, N), dtype=torch.float16, device=device)
    right = torch.empty_like(left)
    out_gate = torch.empty((B, N, N, hidden_dim), dtype=torch.float16, device=device)

    # --------------------------------------------------------------
    # 5) Fused projection / gating / optional mask
    # --------------------------------------------------------------
    BLOCK_M = 64
    BLOCK_H = 64
    BLOCK_K = 32
    grid_proj = (triton.cdiv(M, BLOCK_M), triton.cdiv(hidden_dim, BLOCK_H))

    _proj_gate_mask_kernel[grid_proj](
        x_norm,
        mask_flat,
        left_proj_w_T,
        left_gate_w_T,
        right_proj_w_T,
        right_gate_w_T,
        out_gate_w_T,
        left,
        right,
        out_gate,
        M,
        N,
        dim,
        hidden_dim,
        BLOCK_M=BLOCK_M,
        BLOCK_H=BLOCK_H,
        BLOCK_K=BLOCK_K,
        MASKED=MASKED,
        num_warps=4,
    )

    # --------------------------------------------------------------
    # 6) Pairwise multiplication (batched GEMM) – left @ rightᵀ
    # --------------------------------------------------------------
    left_mat = left.view(B * hidden_dim, N, N)                     # (B*H, N, N)
    right_mat = right.view(B * hidden_dim, N, N).transpose(1, 2)   # (B*H, N, N)ᵀ
    hidden_fp16 = torch.bmm(left_mat, right_mat)                   # (B*H, N, N) fp16
    hidden = hidden_fp16.view(B, hidden_dim, N, N)                # (B, H, N, N) fp16

    # --------------------------------------------------------------
    # 7) Fused hidden‑dim LayerNorm → out‑gate → final linear
    # --------------------------------------------------------------
    to_out_norm_w = weights["to_out_norm.weight"]   # (H,) fp32
    to_out_norm_b = weights["to_out_norm.bias"]    # (H,) fp32
    to_out_w_T = weights["to_out.weight"].t().contiguous().to(torch.float16)  # (H, C)

    out = torch.empty((B, N, N, dim), dtype=torch.float32, device=device)

    BLOCK_M_OUT = 64
    BLOCK_H_OUT = hidden_dim      # cover the whole hidden dim in one kernel launch
    BLOCK_D_OUT = 64

    grid_out = (triton.cdiv(B * N * N, BLOCK_M_OUT),)

    _ln_gate_out_linear_fused_kernel[grid_out](
        hidden.view(-1),                 # flat fp16 hidden
        out_gate.view(-1),               # flat fp16 out‑gate
        to_out_norm_w,
        to_out_norm_b,
        to_out_w_T,
        out,
        B,
        N,
        hidden_dim,
        dim,
        eps,
        BLOCK_M=BLOCK_M_OUT,
        BLOCK_H=BLOCK_H_OUT,
        BLOCK_D=BLOCK_D_OUT,
        num_warps=4,
    )

    return out
</code></pre>
                </div>
            </details>
        </div>

        <div class="domain-section">
            <h3 class="domain-title">Algorithm Engineering</h3>
            <p class="domain-desc">AtCoder Heuristic Contests on real-world optimization. <a href="https://atcoder.jp/contests/ahc039/submissions/72633477" target="_blank">Verify submissions →</a></p>
            <div class="table-wrap">
                <table class="mini-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>AHC39 (Geometry) ↑</th>
                            <th>AHC58 (Scheduling) ↑</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Best Human</td>
                            <td>566,997</td>
                            <td>847,674,723</td>
                        </tr>
                        <tr>
                            <td>Prev. Best AI</td>
                            <td>558,026</td>
                            <td>848,373,282</td>
                        </tr>
                        <tr>
                            <td>TTT-Discover</td>
                            <td>567,062</td>
                            <td>848,414,228</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="domain-section">
            <h3 class="domain-title">Biology</h3>
            <p class="domain-desc">Single-cell RNA-seq denoising on OpenProblems benchmark. <a href="https://github.com/test-time-training/discover/blob/main/results/denoising/" target="_blank">Verify results →</a></p>
            <div class="table-wrap">
                <table class="mini-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>PBMC ↑</th>
                            <th>Tabula ↑</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Best Human</td>
                            <td>0.64</td>
                            <td>0.64</td>
                        </tr>
                        <tr>
                            <td>Prev. Best AI</td>
                            <td class="dim">—</td>
                            <td class="dim">—</td>
                        </tr>
                        <tr>
                            <td>TTT-Discover</td>
                            <td>0.71</td>
                            <td>0.73</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <section style="margin-top: 56px;">
            <p class="label">Cite</p>
            <pre class="bibtex">@article{ttt-discover2026,
  title   = {Learning to Discover at Test Time},
  author  = {Yuksekgonul, Mert and Koceja, Daniel and Li, Xinhao 
             and Bianchi, Federico and McCaleb, Jed and Wang, Xiaolong 
             and Kautz, Jan and Choi, Yejin and Zou, James 
             and Guestrin, Carlos and Sun, Yu},
  journal = {arXiv preprint},
  year    = {2026}
}</pre>
        </section>

        <footer>TTT-Discover 2026</footer>
    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
            });
            hljs.highlightAll();
        });
    </script>
</body>
</html>
